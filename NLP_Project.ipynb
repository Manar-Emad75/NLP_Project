{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+3+GeIbdg7iOFpFTHkqsg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manar-Emad75/NLP_Project/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "bt3WQfCdc2Bf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import zipfile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XT48Uw1pBQ5",
        "outputId": "26023f20-302c-4849-83f3-49403772b80e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = '/content/pos.zip'\n",
        "extracted_folder_path = '/content/extracted_folder'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "\n",
        "zip_file_path = '/content/neg.zip'\n",
        "extracted_folder_path = '/content/extracted_folder'\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "folderNeg_path = '/content/extracted_folder/neg'\n",
        "folderPos_path = '/content/extracted_folder/pos'"
      ],
      "metadata": {
        "id": "bVoBOEHZR6U0"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read text files and put them in list"
      ],
      "metadata": {
        "id": "9Vhgh9yYpdi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_files(folder_path):\n",
        "    texts = []\n",
        "    files = os.listdir(folder_path)\n",
        "    for file_name in files:\n",
        "        if file_name.endswith('.txt'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            with open(file_path, 'r') as f:\n",
        "                text = f.read()\n",
        "                texts.append(text)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "bmqK5-82pWKo"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_folderNeg = read_text_files(folderNeg_path)\n",
        "texts_folderPos = read_text_files(folderPos_path)\n",
        "\n",
        "# access spacifice file in list\n",
        "neg_file = texts_folderNeg[20]"
      ],
      "metadata": {
        "id": "Bp_1jI9Npz9r"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # tokenization and Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
        "\n",
        "    # Joining the lemmatized text back into a single string\n",
        "    processed_text = ' '.join(lemmatized_text)\n",
        "\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "c4ZkUCAiQ5cy"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load data as train(X) and test(Y)"
      ],
      "metadata": {
        "id": "peiOXYkMh9hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(folder):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for file in os.listdir(folder):\n",
        "        with open(os.path.join(folder, file), 'r', encoding='utf-8') as f:\n",
        "            review = f.read()\n",
        "            processed_review = preprocess_text(review)\n",
        "            reviews.append(processed_review)\n",
        "            labels.append(folder.split('/')[-1])  # Extract label from folder name\n",
        "    return reviews, labels"
      ],
      "metadata": {
        "id": "ZE_l6eX3PytN"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_reviews, positive_labels = load_data(folderPos_path)\n",
        "negative_reviews, negative_labels = load_data(folderNeg_path)"
      ],
      "metadata": {
        "id": "72WoOMrmgKQd"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combine all positive and negative file to train model"
      ],
      "metadata": {
        "id": "zk-EVhPbiRY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine positive and negative reviews and labels\n",
        "all_reviews = positive_reviews + negative_reviews\n",
        "all_labels = positive_labels + negative_labels"
      ],
      "metadata": {
        "id": "Yh3M0_TBHvIf"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply TF-IDF"
      ],
      "metadata": {
        "id": "rwUh5Wd3lcnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "#define train and test\n",
        "X = vectorizer.fit_transform(all_reviews)\n",
        "y = all_labels"
      ],
      "metadata": {
        "id": "6JT1mYwDT9vy"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Spliting"
      ],
      "metadata": {
        "id": "tcFpSNzulkUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training and Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Aj3nrm7qUB7A"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Classifier"
      ],
      "metadata": {
        "id": "qFObkHIHluNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test data\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Visualizing Results\n",
        "print(\"\\nAccuracy: {:.2f} %\".format(accuracy_score(y_test, y_pred) * 100))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BN3ESauUG0p",
        "outputId": "17cb2594-2637-4398-9dde-9732b616b201"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 84.00 %\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.86      0.82      0.84       201\n",
            "         pos       0.82      0.86      0.84       199\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test data\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Visualizing Results\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_JTrpH_qgjt",
        "outputId": "81d170a0-f9ea-4099-fcb8-6defd39026a1"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.00%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.85      0.85      0.85       201\n",
            "         pos       0.85      0.85      0.85       199\n",
            "\n",
            "    accuracy                           0.85       400\n",
            "   macro avg       0.85      0.85      0.85       400\n",
            "weighted avg       0.85      0.85      0.85       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#func to colored output"
      ],
      "metadata": {
        "id": "xfFkFb2muG7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Define HTML for displaying text in specified color and style\n",
        "def colored_text(text, color='black', style='normal'):\n",
        "    color_code = {'black': 'black', 'red': 'red', 'green': 'green'}\n",
        "    style_code = {'normal': 'normal', 'bold': 'bold'}\n",
        "\n",
        "    return f\"<span style='color:{color_code[color]}; font-weight:{style_code[style]}'>{text}</span>\""
      ],
      "metadata": {
        "id": "iyvFyviBuF9L"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define if file is positive or negative"
      ],
      "metadata": {
        "id": "3Oh2csMRgQ5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# aplay pre-processing to file\n",
        "processed_review = preprocess_text(neg_file)\n",
        "\n",
        "# Transform the preprocessed review into a TF-IDF vector\n",
        "review_vector = vectorizer.transform([processed_review])\n",
        "\n",
        "# Classify the sentiment using the trained classifier in LogisticRegression\n",
        "sentiment = classifier.predict(review_vector)[0]\n",
        "\n",
        "if sentiment == 'neg':\n",
        "    display(HTML(colored_text('Negative Review', color='red', style='bold')))\n",
        "else:\n",
        "    display(HTML(colored_text('Positive Review', color='green', style='bold')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cmd6REBGgPyT",
        "outputId": "9bb27bd2-6e9f-4e47-b5f9-ca5821e716b9"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style='color:red; font-weight:bold'>Negative Review</span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the model"
      ],
      "metadata": {
        "id": "yv2g3qCFjrpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "model_filename = 'sentiment_classifier_model.pkl'\n",
        "joblib.dump(classifier, model_filename)\n",
        "\n",
        "# Check if the model file exists\n",
        "if os.path.exists(model_filename):\n",
        "    print(\"Model has been saved successfully.\")\n",
        "else:\n",
        "    print(\"Model could not be saved. Please check the file path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTHcy3LOjqZN",
        "outputId": "3f8be5df-42e9-4e94-ab3a-1f3a223f5ac8"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been saved successfully.\n"
          ]
        }
      ]
    }
  ]
}